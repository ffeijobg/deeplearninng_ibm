{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Classification Models with Keras</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build models for classificaiton problems. We will use the popular MNIST dataset, a dataset of images, for a change. \n",
    "\n",
    "The <strong>MNIST database</strong>, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
    "    \n",
    "The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau.\n",
    "\n",
    "Also, this way, will get to compare how conventional neural networks compare to convolutional neural networks, that we will build in the next module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification Models with Keras</h2>\n",
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Use of MNIST database for training various image processing systems</h5>\n",
    "<h5> 2. Build a Neural Network </h5>\n",
    "<h5> 3. Train and Test the Network. </h5>\n",
    "\n",
    "<p>This link will be used by your peers to assess your project. In your web app, your peers will be able to upload an image, which will then be classified using your custom classifier you connected to the web app. Your project will be graded by how accurately your app can classify <b>Fire</b>, <b>Smoke</b> and <b>Neutral (No Fire or Smoke)</b>.<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item312\">Import Keras and Packages</a>      \n",
    "2. <a href=\"#item322\">Build a Neural Network</a>     \n",
    "3. <a href=\"#item332\">Train and Test the Network</a>     \n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item312'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing Keras and some of its modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. \n",
    "# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n",
    "\n",
    "#!pip show numpy\n",
    "#!pip show pandas\n",
    "#!pip show keras\n",
    "#!pip show matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing we images, let's also import the Matplotlib scripting layer in order to view the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras library conveniently includes the MNIST dataset as part of its API. You can check other datasets within the Keras library [here](https://keras.io/datasets/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01). \n",
    "\n",
    "So, let's load the MNIST dataset from the Keras library. The dataset is readily divided into a training set and a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# read the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the number of images in each set. According to the dataset's documentation, we should have 60000 images in X_train and 10000 images in the X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number in the output tuple is the number of images, and the other two numbers are the size of the images in datset. So, each image is 28 pixels by 28 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first image in the training set using Matplotlib's scripting layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b70fb4cfb0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With conventional neural networks, we cannot feed in the image as input as is. So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "# flatten images into one-dimensional vector\n",
    "\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2] # find size of one-dimensional vector\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # flatten training images\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # flatten test images\n",
    "\n",
    "\n",
    "print(num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we start building our model, remember that for classification we need to divide our target variable into categories. We use the to_categorical function from the Keras Utilities package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item322'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification model\n",
    "def classification_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, activation='relu', input_shape=(num_pixels,)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item332'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9436 - loss: 0.1842 - val_accuracy: 0.9656 - val_loss: 0.1141\n",
      "Epoch 2/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9751 - loss: 0.0795 - val_accuracy: 0.9679 - val_loss: 0.0989\n",
      "Epoch 3/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9832 - loss: 0.0526 - val_accuracy: 0.9731 - val_loss: 0.0897\n",
      "Epoch 4/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9862 - loss: 0.0424 - val_accuracy: 0.9762 - val_loss: 0.0810\n",
      "Epoch 5/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9897 - loss: 0.0305 - val_accuracy: 0.9818 - val_loss: 0.0694\n",
      "Epoch 6/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9917 - loss: 0.0262 - val_accuracy: 0.9762 - val_loss: 0.0858\n",
      "Epoch 7/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9925 - loss: 0.0232 - val_accuracy: 0.9820 - val_loss: 0.0814\n",
      "Epoch 8/100\n",
      "1875/1875 - 2s - 1ms/step - accuracy: 0.9931 - loss: 0.0204 - val_accuracy: 0.9798 - val_loss: 0.0838\n",
      "Epoch 9/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9945 - loss: 0.0167 - val_accuracy: 0.9823 - val_loss: 0.0823\n",
      "Epoch 10/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9953 - loss: 0.0149 - val_accuracy: 0.9781 - val_loss: 0.1173\n",
      "Epoch 11/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9948 - loss: 0.0156 - val_accuracy: 0.9805 - val_loss: 0.0960\n",
      "Epoch 12/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9956 - loss: 0.0146 - val_accuracy: 0.9833 - val_loss: 0.0863\n",
      "Epoch 13/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9958 - loss: 0.0136 - val_accuracy: 0.9820 - val_loss: 0.0937\n",
      "Epoch 14/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9963 - loss: 0.0119 - val_accuracy: 0.9787 - val_loss: 0.1233\n",
      "Epoch 15/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 0.9792 - val_loss: 0.1285\n",
      "Epoch 16/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9970 - loss: 0.0102 - val_accuracy: 0.9838 - val_loss: 0.1043\n",
      "Epoch 17/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.9827 - val_loss: 0.1092\n",
      "Epoch 18/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9971 - loss: 0.0102 - val_accuracy: 0.9793 - val_loss: 0.1330\n",
      "Epoch 19/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9968 - loss: 0.0103 - val_accuracy: 0.9825 - val_loss: 0.1171\n",
      "Epoch 20/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9970 - loss: 0.0105 - val_accuracy: 0.9810 - val_loss: 0.1349\n",
      "Epoch 21/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9977 - loss: 0.0080 - val_accuracy: 0.9819 - val_loss: 0.1151\n",
      "Epoch 22/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9974 - loss: 0.0101 - val_accuracy: 0.9822 - val_loss: 0.1205\n",
      "Epoch 23/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9973 - loss: 0.0092 - val_accuracy: 0.9802 - val_loss: 0.1440\n",
      "Epoch 24/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9984 - loss: 0.0067 - val_accuracy: 0.9808 - val_loss: 0.1414\n",
      "Epoch 25/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9972 - loss: 0.0105 - val_accuracy: 0.9852 - val_loss: 0.1139\n",
      "Epoch 26/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9983 - loss: 0.0062 - val_accuracy: 0.9821 - val_loss: 0.1434\n",
      "Epoch 27/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9977 - loss: 0.0098 - val_accuracy: 0.9807 - val_loss: 0.1548\n",
      "Epoch 28/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 0.9805 - val_loss: 0.1738\n",
      "Epoch 29/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9977 - loss: 0.0094 - val_accuracy: 0.9798 - val_loss: 0.1630\n",
      "Epoch 30/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9981 - loss: 0.0076 - val_accuracy: 0.9833 - val_loss: 0.1365\n",
      "Epoch 31/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9980 - loss: 0.0086 - val_accuracy: 0.9833 - val_loss: 0.1434\n",
      "Epoch 32/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9984 - loss: 0.0074 - val_accuracy: 0.9829 - val_loss: 0.1612\n",
      "Epoch 33/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9980 - loss: 0.0080 - val_accuracy: 0.9802 - val_loss: 0.1970\n",
      "Epoch 34/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9981 - loss: 0.0078 - val_accuracy: 0.9821 - val_loss: 0.1647\n",
      "Epoch 35/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 0.9824 - val_loss: 0.1741\n",
      "Epoch 36/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9986 - loss: 0.0062 - val_accuracy: 0.9848 - val_loss: 0.1420\n",
      "Epoch 37/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9980 - loss: 0.0105 - val_accuracy: 0.9820 - val_loss: 0.1694\n",
      "Epoch 38/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0067 - val_accuracy: 0.9814 - val_loss: 0.1910\n",
      "Epoch 39/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9982 - loss: 0.0081 - val_accuracy: 0.9856 - val_loss: 0.1463\n",
      "Epoch 40/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 0.9843 - val_loss: 0.1508\n",
      "Epoch 41/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0067 - val_accuracy: 0.9831 - val_loss: 0.1744\n",
      "Epoch 42/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9980 - loss: 0.0088 - val_accuracy: 0.9856 - val_loss: 0.1664\n",
      "Epoch 43/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9987 - loss: 0.0058 - val_accuracy: 0.9821 - val_loss: 0.1834\n",
      "Epoch 44/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0074 - val_accuracy: 0.9823 - val_loss: 0.1974\n",
      "Epoch 45/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9983 - loss: 0.0095 - val_accuracy: 0.9845 - val_loss: 0.1621\n",
      "Epoch 46/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 0.9847 - val_loss: 0.1733\n",
      "Epoch 47/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 0.9826 - val_loss: 0.1901\n",
      "Epoch 48/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9982 - loss: 0.0096 - val_accuracy: 0.9820 - val_loss: 0.2431\n",
      "Epoch 49/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0048 - val_accuracy: 0.9829 - val_loss: 0.2236\n",
      "Epoch 50/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9983 - loss: 0.0084 - val_accuracy: 0.9832 - val_loss: 0.2116\n",
      "Epoch 51/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9987 - loss: 0.0066 - val_accuracy: 0.9844 - val_loss: 0.2083\n",
      "Epoch 52/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9984 - loss: 0.0077 - val_accuracy: 0.9813 - val_loss: 0.2078\n",
      "Epoch 53/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0047 - val_accuracy: 0.9830 - val_loss: 0.1902\n",
      "Epoch 54/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9988 - loss: 0.0063 - val_accuracy: 0.9833 - val_loss: 0.2379\n",
      "Epoch 55/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9983 - loss: 0.0122 - val_accuracy: 0.9828 - val_loss: 0.2087\n",
      "Epoch 56/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 0.9823 - val_loss: 0.2351\n",
      "Epoch 57/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9984 - loss: 0.0069 - val_accuracy: 0.9836 - val_loss: 0.2118\n",
      "Epoch 58/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9988 - loss: 0.0051 - val_accuracy: 0.9830 - val_loss: 0.2361\n",
      "Epoch 59/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0073 - val_accuracy: 0.9820 - val_loss: 0.2458\n",
      "Epoch 60/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9987 - loss: 0.0074 - val_accuracy: 0.9824 - val_loss: 0.2336\n",
      "Epoch 61/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 0.9849 - val_loss: 0.2379\n",
      "Epoch 62/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9983 - loss: 0.0093 - val_accuracy: 0.9810 - val_loss: 0.2519\n",
      "Epoch 63/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9987 - loss: 0.0078 - val_accuracy: 0.9832 - val_loss: 0.2334\n",
      "Epoch 64/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0041 - val_accuracy: 0.9833 - val_loss: 0.2444\n",
      "Epoch 65/100\n",
      "1875/1875 - 2s - 1ms/step - accuracy: 0.9988 - loss: 0.0075 - val_accuracy: 0.9842 - val_loss: 0.2079\n",
      "Epoch 66/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 0.9843 - val_loss: 0.2317\n",
      "Epoch 67/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9984 - loss: 0.0095 - val_accuracy: 0.9833 - val_loss: 0.2061\n",
      "Epoch 68/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9994 - loss: 0.0036 - val_accuracy: 0.9839 - val_loss: 0.2185\n",
      "Epoch 69/100\n",
      "1875/1875 - 2s - 1ms/step - accuracy: 0.9990 - loss: 0.0072 - val_accuracy: 0.9815 - val_loss: 0.2817\n",
      "Epoch 70/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0055 - val_accuracy: 0.9823 - val_loss: 0.3112\n",
      "Epoch 71/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9985 - loss: 0.0100 - val_accuracy: 0.9831 - val_loss: 0.2707\n",
      "Epoch 72/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9843 - val_loss: 0.2813\n",
      "Epoch 73/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 0.9854 - val_loss: 0.2451\n",
      "Epoch 74/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0058 - val_accuracy: 0.9835 - val_loss: 0.3199\n",
      "Epoch 75/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9989 - loss: 0.0065 - val_accuracy: 0.9801 - val_loss: 0.3150\n",
      "Epoch 76/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0060 - val_accuracy: 0.9846 - val_loss: 0.2746\n",
      "Epoch 77/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9988 - loss: 0.0065 - val_accuracy: 0.9837 - val_loss: 0.3079\n",
      "Epoch 78/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9986 - loss: 0.0092 - val_accuracy: 0.9827 - val_loss: 0.3235\n",
      "Epoch 79/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9834 - val_loss: 0.3182\n",
      "Epoch 80/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0047 - val_accuracy: 0.9834 - val_loss: 0.2811\n",
      "Epoch 81/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0056 - val_accuracy: 0.9844 - val_loss: 0.2966\n",
      "Epoch 82/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0064 - val_accuracy: 0.9836 - val_loss: 0.3035\n",
      "Epoch 83/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9812 - val_loss: 0.3836\n",
      "Epoch 84/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9851 - val_loss: 0.3025\n",
      "Epoch 85/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9993 - loss: 0.0071 - val_accuracy: 0.9827 - val_loss: 0.3308\n",
      "Epoch 86/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0069 - val_accuracy: 0.9837 - val_loss: 0.2933\n",
      "Epoch 87/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0046 - val_accuracy: 0.9838 - val_loss: 0.2786\n",
      "Epoch 88/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9993 - loss: 0.0052 - val_accuracy: 0.9834 - val_loss: 0.3004\n",
      "Epoch 89/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9989 - loss: 0.0071 - val_accuracy: 0.9836 - val_loss: 0.3018\n",
      "Epoch 90/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9993 - loss: 0.0045 - val_accuracy: 0.9826 - val_loss: 0.3651\n",
      "Epoch 91/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9989 - loss: 0.0072 - val_accuracy: 0.9827 - val_loss: 0.3845\n",
      "Epoch 92/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9994 - loss: 0.0043 - val_accuracy: 0.9843 - val_loss: 0.3298\n",
      "Epoch 93/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9994 - loss: 0.0036 - val_accuracy: 0.9848 - val_loss: 0.3275\n",
      "Epoch 94/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0052 - val_accuracy: 0.9854 - val_loss: 0.3520\n",
      "Epoch 95/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0104 - val_accuracy: 0.9840 - val_loss: 0.3456\n",
      "Epoch 96/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0047 - val_accuracy: 0.9829 - val_loss: 0.3519\n",
      "Epoch 97/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9839 - val_loss: 0.3476\n",
      "Epoch 98/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9993 - loss: 0.0041 - val_accuracy: 0.9844 - val_loss: 0.3165\n",
      "Epoch 99/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9992 - loss: 0.0047 - val_accuracy: 0.9835 - val_loss: 0.3445\n",
      "Epoch 100/100\n",
      "1875/1875 - 3s - 1ms/step - accuracy: 0.9990 - loss: 0.0071 - val_accuracy: 0.9844 - val_loss: 0.3731\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = classification_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the accuracy and the corresponding error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9843999743461609% \n",
      " Error: 0.015600025653839111\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just running 10 epochs could actually take over 20 minutes. But enjoy the results as they are getting generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you cannot afford to retrain your model everytime you want to use it, especially if you are limited on computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. To do that, we use the save method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('classification_model.h5')\n",
    "model.save('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model contains multidimensional arrays of data, then models are usually saved as .h5 files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are ready to use your model again, you use the load_model function from <strong>keras.models</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">615,440</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │         \u001b[38;5;34m615,440\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,010\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">694,952</span> (2.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m694,952\u001b[0m (2.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">694,950</span> (2.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m694,950\u001b[0m (2.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model = load_model('classification_model.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "#model = classification_model()\n",
    "\n",
    "# fit the model\n",
    "#pretrained_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = pretrained_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9843999743461609% \n",
      " Error: 0.015600025653839111\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Alex Aklson](https://www.linkedin.com/in/aklson/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *Introduction to Deep Learning & Neural Networks with Keras*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0101EN_Coursera_Week3_LAB2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2019 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
